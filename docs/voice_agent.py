import asyncio
import numpy as np
import sounddevice as sd
from queue import Queue
import threading
import time
from dataclasses import dataclass
from typing import Optional, AsyncGenerator
import io
import wave
import tempfile
import os
import re

 """åœ¨M2 Mac miniï¼ˆ16Gç»Ÿä¸€å†…å­˜ï¼‰ä¸Šç»„åˆå¼æµæ°´çº¿ï¼ˆæœ¬åœ° ASR + æœ¬åœ°å°/ä¸­å‹ LLMâ€MLX + æœ¬åœ° TTSï¼‰ï¼Œå®ç° â€œæˆ‘è¯´ â†’ ASR â†’ LLM æ€è€ƒ â†’ TTS è¯´å›â€çš„æœ¬åœ°å¯¹è¯"""


# MLX imports
import mlx.core as mx
from mlx_lm import load, generate

# ä½¿ç”¨æ ‡å‡†whisper
try:
    import whisper

    WHISPER_AVAILABLE = True
    print("âœ… OpenAI Whisper available")
except ImportError:
    WHISPER_AVAILABLE = False
    print("âš ï¸ OpenAI Whisper not available")

# ç¦ç”¨VADï¼Œä½¿ç”¨ç®€å•èƒ½é‡æ£€æµ‹
VAD_AVAILABLE = False
print("ğŸ”§ ä½¿ç”¨ç®€å•èƒ½é‡æ£€æµ‹ä»£æ›¿WebRTC VAD")

# TTS imports
try:
    import pyttsx3

    SYSTEM_TTS_AVAILABLE = True
    print("âœ… System TTS available")
except ImportError:
    SYSTEM_TTS_AVAILABLE = False
    print("âš ï¸ No TTS available")


@dataclass
class AudioConfig:
    sample_rate: int = 16000
    chunk_size: int = 1024
    channels: int = 1
    silence_timeout: float = 2.5
    min_audio_length: float = 1.0
    max_buffer_seconds: float = 8.0
    energy_threshold: float = 0.005
    silence_threshold: float = 0.001


class StreamingVoiceAgent:
    def __init__(self):
        self.config = AudioConfig()
        self.audio_queue = Queue()
        self.is_listening = False
        self.is_speaking = False

        # Initialize components
        self._init_asr()
        self._init_llm()
        self._init_tts()

    def _init_asr(self):
        """åˆå§‹åŒ–ASR"""
        print("ğŸ¤ Loading ASR model...")
        if WHISPER_AVAILABLE:
            try:
                self.asr_model = whisper.load_model("base")
                self.asr_type = "whisper"
                print("âœ… Whisper model loaded (offline)")
            except Exception as e:
                print(f"âŒ Failed to load Whisper: {e}")
                self.asr_model = None
                self.asr_type = "none"
        else:
            self.asr_model = None
            self.asr_type = "none"
            print("âŒ No ASR available")

    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        print("ğŸ§  Loading LLM model...")
        try:
            self.llm_model, self.tokenizer = load(
                "mlx-community/Qwen2.5-7B-Instruct-4bit",
                tokenizer_config={"trust_remote_code": True}
            )
            print("âœ… LLM model loaded")
        except Exception as e:
            print(f"âŒ Failed to load LLM: {e}")
            raise

    def _init_tts(self):
        """åˆå§‹åŒ–TTS - ä¿®å¤å¤šæ¬¡è°ƒç”¨é—®é¢˜"""
        print("ğŸ”Š Loading TTS model...")

        self.tts = None
        self.tts_type = "none"
        self.tts_lock = asyncio.Lock()  # æ·»åŠ é”é˜²æ­¢å¹¶å‘é—®é¢˜

        if SYSTEM_TTS_AVAILABLE:
            try:
                self.tts = pyttsx3.init()

                # è·å–æ‰€æœ‰å¯ç”¨è¯­éŸ³
                voices = self.tts.getProperty('voices')
                self.available_voices = {}

                if voices:
                    print("ğŸµ å¯ç”¨è¯­éŸ³:")
                    for i, voice in enumerate(voices):
                        voice_id = voice.id
                        voice_name = voice.name
                        lang_code = getattr(voice, 'languages', ['unknown'])[0] if hasattr(voice,
                                                                                           'languages') and voice.languages else 'unknown'

                        print(f"   [{i}] {voice_name} ({lang_code}) - {voice_id}")

                        # å»ºç«‹è¯­è¨€æ˜ å°„
                        if any(x in voice_name.lower() or x in voice_id.lower() for x in
                               ['chinese', 'zh', 'mandarin', 'ä¸­æ–‡']):
                            self.available_voices['zh'] = voice_id
                        elif any(x in voice_name.lower() or x in voice_id.lower() for x in
                                 ['english', 'en', 'us', 'uk']):
                            self.available_voices['en'] = voice_id
                        elif any(x in voice_name.lower() or x in voice_id.lower() for x in ['japanese', 'ja', 'æ—¥æœ¬']):
                            self.available_voices['ja'] = voice_id
                        elif any(x in voice_name.lower() or x in voice_id.lower() for x in ['korean', 'ko', 'í•œêµ­']):
                            self.available_voices['ko'] = voice_id
                        elif any(
                                x in voice_name.lower() or x in voice_id.lower() for x in ['french', 'fr', 'franÃ§ais']):
                            self.available_voices['fr'] = voice_id
                        elif any(x in voice_name.lower() or x in voice_id.lower() for x in ['german', 'de', 'deutsch']):
                            self.available_voices['de'] = voice_id
                        elif any(
                                x in voice_name.lower() or x in voice_id.lower() for x in ['spanish', 'es', 'espaÃ±ol']):
                            self.available_voices['es'] = voice_id

                    print(f"ğŸŒ è¯­è¨€æ˜ å°„: {self.available_voices}")

                    # è®¾ç½®é»˜è®¤è¯­éŸ³ï¼ˆä¼˜å…ˆä¸­æ–‡ï¼‰
                    if 'zh' in self.available_voices:
                        self.tts.setProperty('voice', self.available_voices['zh'])
                        self.current_voice = 'zh'
                        print(f"âœ… é»˜è®¤è¯­éŸ³è®¾ä¸ºä¸­æ–‡")
                    elif 'en' in self.available_voices:
                        self.tts.setProperty('voice', self.available_voices['en'])
                        self.current_voice = 'en'
                        print(f"âœ… é»˜è®¤è¯­éŸ³è®¾ä¸ºè‹±æ–‡")
                    else:
                        self.current_voice = 'default'
                        print("âœ… ä½¿ç”¨ç³»ç»Ÿé»˜è®¤è¯­éŸ³")
                else:
                    self.current_voice = 'default'
                    self.available_voices = {}

                # è®¾ç½®TTSå‚æ•°
                self.tts.setProperty('rate', 180)
                self.tts.setProperty('volume', 0.9)
                self.tts_type = "system"
                print("âœ… System TTS loaded with multi-language support")
                return

            except Exception as e:
                print(f"âš ï¸ System TTS failed: {e}")

        print("âš ï¸ No TTS available - text-only mode")

    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
            korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))
            total_chars = len(text)

            if total_chars == 0:
                return 'en'

            # è®¡ç®—å„è¯­è¨€å­—ç¬¦å æ¯”
            chinese_ratio = chinese_chars / total_chars
            japanese_ratio = japanese_chars / total_chars
            korean_ratio = korean_chars / total_chars

            if chinese_ratio > 0.3:
                return 'zh'
            elif japanese_ratio > 0.3:
                return 'ja'
            elif korean_ratio > 0.3:
                return 'ko'
            else:
                # æ£€æµ‹å…¶ä»–æ¬§æ´²è¯­è¨€çš„ç‰¹å¾è¯
                text_lower = text.lower()
                if any(word in text_lower for word in
                       ['le', 'la', 'les', 'de', 'du', 'des', 'je', 'tu', 'nous', 'vous']):
                    return 'fr'
                elif any(
                        word in text_lower for word in ['der', 'die', 'das', 'ich', 'du', 'wir', 'ihr', 'und', 'oder']):
                    return 'de'
                elif any(word in text_lower for word in ['el', 'la', 'los', 'las', 'yo', 'tu', 'nosotros', 'vosotros']):
                    return 'es'
                else:
                    return 'en'

        except Exception as e:
            print(f"Language detection error: {e}")
            return 'en'

    def _set_voice_for_language(self, language: str):
        """æ ¹æ®è¯­è¨€è®¾ç½®åˆé€‚çš„è¯­éŸ³"""
        if not self.tts or not self.available_voices:
            return

        try:
            target_voice = None

            # ç›´æ¥åŒ¹é…
            if language in self.available_voices:
                target_voice = self.available_voices[language]
            # å›é€€ç­–ç•¥
            elif language in ['zh-cn', 'zh-tw'] and 'zh' in self.available_voices:
                target_voice = self.available_voices['zh']
            elif language in ['en-us', 'en-gb'] and 'en' in self.available_voices:
                target_voice = self.available_voices['en']
            # é»˜è®¤å›é€€åˆ°è‹±æ–‡æˆ–ä¸­æ–‡
            elif 'zh' in self.available_voices:
                target_voice = self.available_voices['zh']
            elif 'en' in self.available_voices:
                target_voice = self.available_voices['en']

            if target_voice and target_voice != getattr(self, 'current_voice_id', None):
                self.tts.setProperty('voice', target_voice)
                self.current_voice_id = target_voice
                print(f"ğŸµ åˆ‡æ¢è¯­éŸ³: {language} -> {target_voice}")

        except Exception as e:
            print(f"Voice switching error: {e}")

    def _energy_vad(self, audio_data: np.ndarray) -> bool:
        """åŸºäºèƒ½é‡çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        rms = np.sqrt(np.mean(audio_data ** 2))
        return rms > self.config.energy_threshold

    def _is_silence(self, audio_data: np.ndarray) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºé™éŸ³"""
        rms = np.sqrt(np.mean(audio_data ** 2))
        return rms < self.config.silence_threshold

    def _audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘å›è°ƒ"""
        if status:
            print(f"Audio input error: {status}")

        if not self.is_speaking:
            audio_data = indata[:, 0].copy()
            audio_data = np.clip(audio_data, -1.0, 1.0)
            self.audio_queue.put(audio_data)

    def _transcribe_audio_whisper(self, audio_data: np.ndarray) -> str:
        """ä½¿ç”¨ç¦»çº¿Whisperè½¬å½•"""
        try:
            if self.asr_model is None:
                return ""

            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)

            audio_data = np.clip(audio_data, -1.0, 1.0)

            result = self.asr_model.transcribe(
                audio_data,
                language=None,
                task="transcribe",
                verbose=False
            )

            return result.get("text", "").strip()

        except Exception as e:
            print(f"Whisper transcription error: {e}")
            return ""

    async def _stream_asr(self) -> AsyncGenerator[str, None]:
        """æµå¼ASRå¤„ç†"""
        buffer = []
        silence_start = None
        last_speech_time = time.time()
        speech_detected = False

        print("ğŸ¤ å¼€å§‹ç›‘å¬éŸ³é¢‘... (ä½¿ç”¨èƒ½é‡æ£€æµ‹)")

        while self.is_listening:
            try:
                current_time = time.time()

                chunks_this_cycle = 0
                while not self.audio_queue.empty() and chunks_this_cycle < 10:
                    try:
                        chunk = self.audio_queue.get_nowait()
                        buffer.append(chunk)
                        chunks_this_cycle += 1
                    except:
                        break

                if len(buffer) > 0:
                    recent_chunks = buffer[-5:]
                    if recent_chunks:
                        recent_audio = np.concatenate(recent_chunks)

                        has_speech = self._energy_vad(recent_audio)
                        is_silence = self._is_silence(recent_audio)

                        if has_speech:
                            if not speech_detected:
                                print("ğŸ”Š æ£€æµ‹åˆ°è¯­éŸ³...")
                                speech_detected = True
                            silence_start = None
                            last_speech_time = current_time
                        elif speech_detected and is_silence:
                            if silence_start is None:
                                silence_start = current_time
                                print("ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³...")

                buffer_duration = len(buffer) * self.config.chunk_size / self.config.sample_rate

                should_process = False
                if (speech_detected and silence_start and
                        current_time - silence_start > self.config.silence_timeout):
                    should_process = True
                elif buffer_duration > self.config.max_buffer_seconds:
                    should_process = True
                    print("âš ï¸ Bufferæ»¡ï¼Œå¼ºåˆ¶å¤„ç†")

                if should_process and buffer_duration > self.config.min_audio_length:
                    print(f"ğŸ”„ å¤„ç†éŸ³é¢‘: {buffer_duration:.1f}s")

                    try:
                        audio_data = np.concatenate(buffer)
                        rms = np.sqrt(np.mean(audio_data ** 2))
                        print(f"ğŸ“Š éŸ³é¢‘RMS: {rms:.4f}")

                        if rms > self.config.silence_threshold:
                            if self.asr_type == "whisper":
                                text = self._transcribe_audio_whisper(audio_data)
                            else:
                                text = ""

                            if text and len(text.strip()) > 2:
                                print(f"ğŸ¤ è½¬å½•: {text}")
                                yield text
                            else:
                                print("ğŸ”‡ æ— æœ‰æ•ˆè¯­éŸ³å†…å®¹")
                        else:
                            print("ğŸ”‡ éŸ³é¢‘èƒ½é‡è¿‡ä½ï¼Œè·³è¿‡")

                    except Exception as e:
                        print(f"Audio processing error: {e}")

                    buffer = []
                    silence_start = None
                    speech_detected = False

                if current_time - last_speech_time > 15:
                    if buffer:
                        print("ğŸ§¹ æ¸…ç†è¶…æ—¶buffer")
                        buffer = []
                    silence_start = None
                    speech_detected = False

                await asyncio.sleep(0.05)

            except Exception as e:
                print(f"Stream ASR error: {e}")
                await asyncio.sleep(0.1)

    async def _generate_response(self, text: str) -> AsyncGenerator[str, None]:
        """ç”ŸæˆLLMå“åº”"""
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå›å¤æ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚"},
            {"role": "user", "content": text}
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        try:
            print("ğŸ§  ç”Ÿæˆå›å¤...")

            response = generate(
                model=self.llm_model,
                tokenizer=self.tokenizer,
                prompt=prompt,
                max_tokens=128,
            )

            response = response.strip()

            if prompt in response:
                response = response.replace(prompt, "").strip()

            response = response.replace("assistant\n", "").replace("Assistant:", "").replace("AI:", "").strip()

            lines = [line.strip() for line in response.split('\n') if line.strip()]
            if lines:
                response = lines[0]

            if response and len(response) > 3:
                yield response
            else:
                yield "æˆ‘æ˜ç™½äº†ï¼Œè¿˜æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"

        except Exception as e:
            print(f"LLM generation error: {e}")
            yield "æˆ‘ç†è§£äº†ä½ çš„é—®é¢˜ï¼Œè®©æˆ‘æ¢ä¸ªæ–¹å¼å›ç­”ä½ ã€‚"

    async def _stream_tts(self, text: str):
        """TTSåˆæˆä¸æ’­æ”¾ - ä¿®å¤å¤šè¯­è¨€æ”¯æŒå’Œé‡å¤æ’­æ”¾é—®é¢˜"""
        if not self.tts:
            print(f"ğŸ’¬ AIå›å¤: {text}")
            return

        # ä½¿ç”¨å¼‚æ­¥é”é˜²æ­¢å¹¶å‘TTSè°ƒç”¨
        async with self.tts_lock:
            try:
                print(f"ğŸ”Š è¯­éŸ³æ’­æ”¾: {text}")

                # æ£€æµ‹è¯­è¨€å¹¶åˆ‡æ¢è¯­éŸ³
                detected_lang = self._detect_language(text)
                print(f"ğŸŒ æ£€æµ‹è¯­è¨€: {detected_lang}")
                self._set_voice_for_language(detected_lang)

                self.is_speaking = True

                # åˆ†å¥æ’­æ”¾ï¼Œæ”¹è¿›åˆ†å¥é€»è¾‘
                sentences = self._split_sentences(text)
                print(f"ğŸ“ åˆ†å¥ç»“æœ: {sentences}")

                for i, sentence in enumerate(sentences):
                    if sentence.strip():
                        print(f"ğŸµ æ’­æ”¾ç¬¬{i + 1}å¥: {sentence}")

                        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡ŒTTSé¿å…é˜»å¡
                        def speak_sentence():
                            try:
                                # é‡æ–°åˆå§‹åŒ–TTSå¼•æ“ä»¥é¿å…çŠ¶æ€é—®é¢˜
                                temp_tts = pyttsx3.init()

                                # é‡æ–°è®¾ç½®è¯­éŸ³
                                if hasattr(self, 'current_voice_id'):
                                    temp_tts.setProperty('voice', self.current_voice_id)
                                temp_tts.setProperty('rate', 180)
                                temp_tts.setProperty('volume', 0.9)

                                temp_tts.say(sentence)
                                temp_tts.runAndWait()
                                temp_tts.stop()

                            except Exception as e:
                                print(f"TTS sentence error: {e}")

                        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(None, speak_sentence)

                        # å¥å­é—´çŸ­æš‚åœé¡¿
                        if i < len(sentences) - 1:
                            await asyncio.sleep(0.3)

                print("âœ… TTSæ’­æ”¾å®Œæˆ")
                self.is_speaking = False

            except Exception as e:
                print(f"TTS error: {e}")
                self.is_speaking = False

    def _split_sentences(self, text: str) -> list:
        """æ”¹è¿›çš„åˆ†å¥æ–¹æ³•"""
        if not text.strip():
            return []

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å¥ï¼Œæ”¯æŒä¸­è‹±æ–‡æ ‡ç‚¹
        import re

        # åˆ†å¥æ ‡ç‚¹ç¬¦å·
        sentence_endings = r'[ã€‚ï¼ï¼Ÿ.!?]+\s*'
        sentences = re.split(sentence_endings, text)

        # è¿‡æ»¤ç©ºå¥å­å¹¶ä¿ç•™æ ‡ç‚¹
        result = []
        parts = re.findall(r'[^ã€‚ï¼ï¼Ÿ.!?]*[ã€‚ï¼ï¼Ÿ.!?]+|[^ã€‚ï¼ï¼Ÿ.!?]+$', text)

        for part in parts:
            part = part.strip()
            if part:
                result.append(part)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†å¥ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬
        if not result:
            result = [text.strip()]

        return result

    async def _conversation_loop(self):
        """ä¸»å¯¹è¯å¾ªç¯"""
        print("\nğŸ¯ è¯­éŸ³å¯¹è¯å·²å¯åŠ¨!")
        print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   - æ­£å¸¸è¯´è¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹è¯­éŸ³å¼€å§‹å’Œç»“æŸ")
        print("   - è¯´å®Œè¯åç­‰å¾…2.5ç§’ï¼ŒAIä¼šè‡ªåŠ¨å¤„ç†")
        print("   - æ”¯æŒä¸­è‹±æ–‡åŠå¤šè¯­è¨€æ··åˆè¯†åˆ«å’Œæ’­æŠ¥")
        print("   - æŒ‰ Ctrl+C é€€å‡º")
        print("=" * 50)

        async for user_text in self._stream_asr():
            if not user_text.strip():
                continue

            print(f"\nğŸ‘¤ ç”¨æˆ·: {user_text}")

            # æš‚åœå½•éŸ³
            self.is_listening = False
            print("â¸ï¸ æš‚åœå½•éŸ³ï¼ŒAIæ€è€ƒä¸­...")

            # ç”Ÿæˆå¹¶æ’­æ”¾å›å¤
            async for response in self._generate_response(user_text):
                print(f"ğŸ¤– AI: {response}")
                await self._stream_tts(response)

            print("=" * 50)

            # æ¢å¤å½•éŸ³
            await asyncio.sleep(1.5)
            self.is_listening = True
            print("ğŸ¤ ç»§ç»­ç›‘å¬...")

    def start_conversation(self):
        """å¯åŠ¨è¯­éŸ³å¯¹è¯"""
        print("ğŸš€ å¯åŠ¨è¯­éŸ³Agent...")
        print(f"ğŸ”§ ç³»ç»Ÿé…ç½®:")
        print(f"   - ASR: {self.asr_type}")
        print(f"   - LLM: Qwen2.5-7B-Instruct (4bit)")
        print(f"   - TTS: {self.tts_type}")
        print(f"   - VAD: èƒ½é‡æ£€æµ‹ (é˜ˆå€¼: {self.config.energy_threshold})")
        print(f"   - é™éŸ³è¶…æ—¶: {self.config.silence_timeout}s")

        if self.asr_model is None:
            print("âŒ ASRä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨")
            return

        # æµ‹è¯•LLM
        print("ğŸ”§ æµ‹è¯•LLMç”ŸæˆåŠŸèƒ½...")
        try:
            test_prompt = self.tokenizer.apply_chat_template(
                [{"role": "user", "content": "ä½ å¥½"}],
                tokenize=False,
                add_generation_prompt=True
            )
            test_response = generate(
                model=self.llm_model,
                tokenizer=self.tokenizer,
                prompt=test_prompt,
                max_tokens=50
            )
            print("âœ… LLMæµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
            return

        self.is_listening = True

        try:
            with sd.InputStream(
                    channels=self.config.channels,
                    samplerate=self.config.sample_rate,
                    blocksize=self.config.chunk_size,
                    callback=self._audio_callback,
                    dtype=np.float32
            ):
                asyncio.run(self._conversation_loop())
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å¯¹è¯ç»“æŸ")
            self.is_listening = False
        except Exception as e:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥:")
    print(f"   - MLXè®¾å¤‡: {mx.default_device()}")
    print(f"   - Whisperå¯ç”¨: {WHISPER_AVAILABLE}")
    print(f"   - TTSå¯ç”¨: {SYSTEM_TTS_AVAILABLE}")

    if not WHISPER_AVAILABLE:
        print("\nğŸš¨ éœ€è¦å®‰è£…Whisper:")
        print("pip install openai-whisper")
        exit(1)

    agent = StreamingVoiceAgent()
    agent.start_conversation()
